
# Apache Spark Quick Start com PySpark (Explicado em Python)

Este guia resume a seção [Quick Start](https://spark.apache.org/docs/latest/quick-start.html) da documentação do Apache Spark, com explicações profundas e exemplos funcionais usando **PySpark**.

---

## 1. Iniciando com PySpark

### Criando uma SparkSession

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("QuickStartExample") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
```

- `SparkSession` é o ponto de entrada para o PySpark.
- `.appName()` define o nome da aplicação.
- `.config()` permite ajustes personalizados.
- `.getOrCreate()` retorna a sessão atual ou cria uma nova.

---

## 2. Criando um DataFrame

```python
dados = [
    ("Alice", 29, "RJ"),
    ("Bruno", 34, "SP"),
    ("Carla", 22, "MG")
]

colunas = ["nome", "idade", "estado"]
df = spark.createDataFrame(dados, schema=colunas)
df.show()
df.printSchema()
```

- Criação de `DataFrame` com schema explícito.
- `.show()` exibe os dados.
- `.printSchema()` mostra os tipos das colunas.

---

## 3. Operações com DataFrames

```python
df.filter(df.idade > 25).show()
df.select("nome", "estado").show()

from pyspark.sql.functions import avg
df.groupBy("estado").agg(avg("idade").alias("idade_media")).show()
```

- `.filter()` aplica condições.
- `.select()` projeta colunas.
- `.groupBy()` com `.agg()` permite agregações.

---

## 4. Caching (Armazenamento em Memória)

```python
df.cache()
df.count()
df.show()
```

- `.cache()` marca o DataFrame para cache em memória.
- Ideal quando reutilizaremos o mesmo resultado várias vezes.

---

## 5. Script PySpark Autônomo

Crie `quick_start_spark.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import avg

def main():
    spark = SparkSession.builder.appName("QuickStartApp").getOrCreate()

    dados = [("Ana", 28, "SP"), ("Bia", 31, "RJ"), ("Carlos", 19, "MG")]
    colunas = ["nome", "idade", "estado"]
    df = spark.createDataFrame(dados, schema=colunas)

    df.show()
    df.printSchema()

    df.filter(df.idade >= 21).show()
    df.groupBy("estado").agg(avg("idade").alias("idade_media")).show()

    df.cache()
    df.count()

    spark.stop()

if __name__ == "__main__":
    main()
```

Execute com:

```bash
spark-submit quick_start_spark.py
```

---

## Resumo das Etapas

| Etapa               | Descrição |
|---------------------|-----------|
| SparkSession        | Criação e configuração do ambiente Spark |
| Criação de DataFrame| A partir de dados locais e schema |
| Operações comuns    | Filtrar, selecionar, agregar |
| Caching             | Otimização de performance |
| Script standalone   | Execução em produção com `spark-submit` |

---

Para continuar o aprendizado, recomenda-se seguir para Structured Streaming, MLlib ou Spark SQL.
