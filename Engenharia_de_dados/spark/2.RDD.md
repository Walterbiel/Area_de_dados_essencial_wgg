
# Guia Completo de Programa√ß√£o com RDD no PySpark

Este guia √© baseado na [documenta√ß√£o oficial do Apache Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html) com foco em **RDDs no PySpark**, trazendo **explica√ß√µes profundas**, **exemplos funcionais** e **exerc√≠cios t√©cnicos**.

---

## üìå O que √© um RDD?

**RDD (Resilient Distributed Dataset)** √© a estrutura de dados fundamental do Apache Spark. Trata-se de uma cole√ß√£o **imut√°vel**, **distribu√≠da** e **tolerante a falhas**, que pode ser processada em paralelo.

### ‚úÖ Por que usar RDD?
- Para pipelines de dados com transforma√ß√µes complexas ou iterativas.
- Quando o controle total da execu√ß√£o √© necess√°rio.
- Para processar dados n√£o tabulares ou n√£o estruturados.

### üìà Aplica√ß√µes comuns
- Processamento de logs.
- Contagem de palavras.
- C√°lculo distribu√≠do de estat√≠sticas.
- Algoritmos baseados em grafos ou itera√ß√£o.

---

## üß™ Iniciando com SparkContext

```python
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("RDDGuideExample").setMaster("local[*]")
sc = SparkContext(conf=conf)
```

- `SparkConf()` configura o nome e ambiente de execu√ß√£o.
- `SparkContext` √© o ponto de entrada para opera√ß√µes com RDDs.

---

## üî® Criando RDDs

### 1. A partir de cole√ß√µes locais (`parallelize`)

```python
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data, numSlices=2)
```

- Cria um RDD a partir de uma lista Python.
- `numSlices=2` indica duas parti√ß√µes (execu√ß√£o paralela).

### 2. A partir de arquivos (`textFile`)

```python
rdd = sc.textFile("dados.txt")
```

- L√™ arquivo texto linha a linha como RDD de strings.

---

## üîÅ 3. Transforma√ß√µes e ‚ö° A√ß√µes

RDDs s√£o transformados com **transforma√ß√µes** (que s√£o *lazy*) e executados com **a√ß√µes**.

### üîÑ Transforma√ß√µes

```python
rdd2 = rdd.map(lambda x: x * 2)
```

- `.map(func)` aplica `func` a cada elemento do RDD.
- Retorna um novo RDD com os resultados.

```python
rdd3 = rdd2.filter(lambda x: x > 5)
```

- `.filter(func)` mant√©m apenas os elementos em que `func` retorna `True`.

```python
rdd4 = rdd3.flatMap(lambda x: [x, x*10])
```

- `.flatMap()` √© como `map()`, mas "achata" os resultados se forem iter√°veis.

```python
rdd5 = rdd3.distinct()
```

- `.distinct()` remove duplicatas do RDD.

### ‚ö° A√ß√µes

```python
rdd3.collect()
```

- Retorna **todos os elementos** do RDD para o driver (cuidado com grandes volumes).

```python
rdd3.count()
```

- Conta o n√∫mero de elementos no RDD.

```python
rdd3.take(3)
```

- Retorna os **3 primeiros elementos** do RDD.

```python
rdd3.first()
```

- Retorna o **primeiro elemento**.

```python
rdd3.reduce(lambda a, b: a + b)
```

- Combina todos os elementos usando uma fun√ß√£o bin√°ria.

---

## üíæ Persist√™ncia e Cache

```python
rdd.cache()
rdd.count()
```

- `.cache()` armazena o RDD em mem√≥ria ap√≥s a primeira a√ß√£o.
- Evita recomputa√ß√µes dispendiosas em DAGs longos.

```python
rdd.persist(StorageLevel.DISK_ONLY)
```

- `.persist()` permite n√≠veis como `MEMORY_AND_DISK`, `DISK_ONLY`, etc.

---

## üîë RDDs de Pares (Key-Value)

```python
pares = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
```

### Agrega√ß√µes por chave

```python
pares.reduceByKey(lambda a, b: a + b).collect()
```

- Agrupa por chave e aplica soma.

```python
pares.groupByKey().mapValues(list).collect()
```

- Agrupa os valores por chave, retorna iterador.

```python
pares.sortByKey().collect()
```

- Ordena as chaves.

---

## üîÑ Shuffle (Reorganiza√ß√£o de dados)

- Opera√ß√µes como `reduceByKey`, `groupByKey`, `join`, `distinct` geram *shuffle*, ou seja, redistribui√ß√£o entre parti√ß√µes.
- **Custo alto** ‚Üí Evitar quando poss√≠vel.
- Prefira `reduceByKey` (agrega localmente antes de embaralhar) em vez de `groupByKey`.

---

## üåê Vari√°veis Compartilhadas

### Broadcast

```python
valores = sc.broadcast([2, 4, 6, 8])
rdd.map(lambda x: x in valores.value).collect()
```

- Envia dados para todos os workers de forma eficiente.

### Accumulator

```python
acc = sc.accumulator(0)
rdd.foreach(lambda x: acc.add(x))
print(acc.value)
```

- Acumuladores s√£o √∫teis para contagens, somas, m√©tricas.

---

## üß© Exemplo Completo

```python
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("ExemploCompleto").setMaster("local[*]")
sc = SparkContext(conf=conf)

dados = sc.parallelize(range(1, 11))  # RDD com 1 a 10

# Transforma√ß√µes
quadrados = dados.map(lambda x: x ** 2)
pares = quadrados.filter(lambda x: x % 2 == 0)

# A√ß√µes
print("Quadrados pares:", pares.collect())
print("Soma:", pares.reduce(lambda a, b: a + b))

# Key-Value
palavras = sc.parallelize(["uva", "banana", "uva", "ma√ß√£", "banana"])
kv = palavras.map(lambda x: (x, 1))
frequencia = kv.reduceByKey(lambda a, b: a + b)
print("Contagem de palavras:", frequencia.collect())

# Broadcast e Accumulator
ref = sc.broadcast(set([4, 16, 36, 64]))
acc = sc.accumulator(0)

def contar(x):
    if x in ref.value:
        acc.add(1)
    return x

_ = quadrados.map(contar).collect()
print("Presentes no broadcast:", acc.value)

sc.stop()
```

---

## üß† Exerc√≠cios T√©cnicos

### üî¢ 1. Soma de quadrados pares
- Crie um RDD com n√∫meros de 1 a 100.
- Filtre apenas os pares, calcule os quadrados e some tudo.

### üìö 2. Contagem de palavras com stopwords
- Use `textFile()` para carregar um texto.
- Divida em palavras com `flatMap`.
- Filtre as stopwords com `broadcast`.
- Use `reduceByKey` para contar.

### üè™ 3. Agrega√ß√£o de vendas
- Dados: `[("lojaA", 100), ("lojaB", 200), ("lojaA", 50)]`
- Calcule o total por loja com `reduceByKey()`.
- Ordene decrescentemente.

---

## ‚úÖ Conclus√£o

RDDs s√£o ideais quando:
- Voc√™ precisa de controle baixo n√≠vel da execu√ß√£o distribu√≠da.
- Est√° lidando com dados n√£o estruturados ou com l√≥gica personalizada.
- Frameworks de alto n√≠vel como DataFrame n√£o s√£o suficientes.

